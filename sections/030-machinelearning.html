
<!-- Machine-Learning -->
    <section id="machinelearning" class="wrapper style2 spotlights">
        <div class="inner">
            <h2>Large scale Machine Learning</h2>
            <p> Millions of degree of freedom. </p>
        </div>
        <section>
            <div class="content">
                <div class="inner">
                    <h2>Optimization: L-BFGS</h2>
		    <img src="http://aria42.com/images/bfgs.png" width="70%" alt>
		    <p>
		    The <b>BFGS</b> (Broyden, Fletcher, Goldfarb, Shanno) algorithm is a quasi-Newton algorithm for nonlinear optimization. By using the previous iterations it calculates an approximation to the inverse Hessian. However, storing all of the previous iterations can require a lot of memory (for millions or billions of dimensions). <b>L-BFGS</b> stands for "Limited memory BFGS", and stores only the last few iterations, but still works (almost) as well.
		    </p>
                    <ul class="actions">
                        <li><a href="http://aria42.com/blog/2014/12/understanding-lbfgs" class="button" target = "_blank">Learn more</a></li>
                    </ul>
                </div>
            </div>
        </section>
    </section>
